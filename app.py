## .env.example
OPENAI_API_KEY=your-openai-api-key
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_ENV=your-pinecone-environment

---

## requirements.txt
openai
langchain
pinecone-client
streamlit
faiss-cpu
python-dotenv
guardrails-ai
tqdm
sentence-transformers

---

## ingest/ingest_docs.py
import os
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from dotenv import load_dotenv

load_dotenv()

def ingest():
    loader = DirectoryLoader("data")
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(docs, embeddings)
    db.save_local("faiss_index")

if __name__ == "__main__":
    ingest()

---

## retriever/faiss_retriever.py
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

def load_retriever():
    embeddings = OpenAIEmbeddings()
    db = FAISS.load_local("faiss_index", embeddings)
    return db

---

## llm/rag_chain.py
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from retriever.faiss_retriever import load_retriever

retriever = load_retriever()
llm = ChatOpenAI(temperature=0)
rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

def ask_rag(query):
    return rag_chain.run(query)

---

## guardrails/safety_rails.gr.xml
<guardrails>
  <output>
    <type>string</type>
    <constraints>
      <length max="1000" />
      <profanity level="moderate" />
    </constraints>
  </output>
</guardrails>

---

## guardrails/eval_feedback.py
from guardrails import Guard
from llm.rag_chain import ask_rag

guard = Guard.from_preset("guardrails/safety_rails.gr.xml")

def guarded_ask(query):
    raw_output = ask_rag(query)
    validated_output = guard.validate(output=raw_output)
    return validated_output.output

---

## app/ui.py
import streamlit as st
from guardrails.eval_feedback import guarded_ask

st.set_page_config(page_title="RAG Product Starter")
st.title("üîç RAG Q&A over Your Docs")

query = st.text_input("Ask a question")

if st.button("Run") and query:
    with st.spinner("Thinking..."):
        answer = guarded_ask(query)
        st.write("### Answer:", answer)

---

## utils/config.py
import os
from dotenv import load_dotenv

load_dotenv()

def get_env_var(key):
    return os.getenv(key)

---

## README.md
# üß† GenAI RAG Product Starter Kit

Build your own GenAI RAG system in under 30 minutes, product-style. Includes vector DB, retrieval, LLM response, guardrails, and Streamlit UI.

## üîß Stack
- FAISS (or Pinecone)
- LangChain + OpenAI
- GuardrailsAI
- Streamlit UI
- Replit-ready

## üõ†Ô∏è Setup
```bash
git clone https://github.com/yourname/rag-product-starter.git
cd rag-product-starter
pip install -r requirements.txt
cp .env.example .env

# Add files to /data then ingest
data/mydoc.pdf
python ingest/ingest_docs.py

# Run app
streamlit run app/ui.py
```

## üîÅ Customization
- Swap FAISS with Pinecone in `retriever/`
- Adjust Guardrails in `guardrails/`
- Extend with agents in `agents/`
